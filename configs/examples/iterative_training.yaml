# Iterative Training Experiment Configuration
#
# This config runs iterative training with periodic merging.
# Usage: python merginguriel/run_large_scale_iterative_training.py --config configs/examples/iterative_training.yaml

# Target languages (null = all available)
target_languages: null

# Or specify specific locales:
# target_languages:
#   - "sq-AL"
#   - "th-TH"

# Merging mode for iterative training
mode: "similarity"

# Target configuration
target:
  locale: ""           # Set dynamically per run
  inclusion: "ExcTar"

# Similarity configuration
similarity:
  type: "URIEL"
  source: "dense"
  top_k: 20
  sinkhorn_iters: 20

# Model configuration
model:
  base_model: "xlm-roberta-base"
  models_root: "haryos_model"
  num_languages: 5

# Dataset configuration
dataset:
  name: "AmazonScience/massive"
  split: "train"
  text_column: "utt"
  label_column: "label"

# Output configuration
output:
  results_dir: "results"
  merged_models_dir: "merged_models"
  cleanup_after_eval: false

# Training hyperparameters
training:
  epochs: 15
  learning_rate: 5e-5
  batch_size: 32         # Reduced for sequential training
  max_seq_length: 128
  bf16: true             # Use bfloat16 for performance
  gradient_accumulation_steps: 1
  warmup_ratio: 0.1
  weight_decay: 0.01
  save_strategy: "epoch"
  eval_strategy: "epoch"
  early_stopping_patience: 3

# Merge configuration
merge:
  frequency: 3                # Merge every 3 epochs
  algorithm: "linear"         # linear, fisher_simple, fisher_dataset
  weight_calculation: "similarity"
  checkpoint_before_merge: true
  retain_checkpoints: 3

# Orchestration settings
sequential_training: true     # Train models one by one (prevents OOM)
enable_wandb: false           # Disable W&B logging
wandb_mode: "disabled"        # online, offline, disabled

# Experiment control
resume: true
start_from: 0
max_locales: null
max_models: 5                 # Maximum models per locale
timeout_hours: 12.0
cleanup_intermediate: false
