{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Model Merging Experiment\n",
    "\n",
    "This notebook demonstrates how to perform a quick model merging experiment using the `MergingUriel` codebase. \n",
    "We will:\n",
    "1.  Select a target language and source models.\n",
    "2.  Compute similarity weights (the \"smarter way\").\n",
    "3.  Merge the models.\n",
    "4.  Evaluate the merged model on the MASSIVE dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup paths to ensure imports work correctly\n",
    "project_root = os.path.abspath(os.getcwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "submodule_path = os.path.join(project_root, 'submodules/auto_merge_llm')\n",
    "if submodule_path not in sys.path:\n",
    "    sys.path.insert(0, submodule_path)\n",
    "\n",
    "# Import merginguriel components\n",
    "from merginguriel.run_merging_pipeline_refactored import (\n",
    "    MergeConfig, \n",
    "    WeightCalculatorFactory, \n",
    "    ModelMerger, \n",
    "    OutputManager, \n",
    "    Evaluator\n",
    ")\n",
    "from merginguriel.evaluate_specific_model import evaluate_specific_model, save_evaluation_results, create_results_folder\n",
    "from merginguriel.naming_config import naming_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set your experiment parameters here. \n",
    "You can choose the target language (locale), the similarity type ('URIEL' or 'REAL'), and the merge mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment Parameters ---\n",
    "TARGET_LANG = \"sq-AL\"  # Example: Albanian\n",
    "MERGE_MODE = \"similarity\" # Options: 'similarity', 'average', 'fisher', 'ties', 'task_arithmetic', 'slerp'\n",
    "SIMILARITY_TYPE = \"URIEL\" # Options: 'URIEL' (linguistic) or 'REAL' (empirical)\n",
    "NUM_LANGUAGES = 5      # Number of top-k similar languages to merge\n",
    "BASE_MODEL_TYPE = \"xlm-roberta-base\" # or 'xlm-roberta-large'\n",
    "INCLUDE_TARGET = False # Set to True to include the target language model itself in the merge (IT mode)\n",
    "\n",
    "# Ensure models directory exists\n",
    "MODELS_ROOT = \"haryos_model\" if BASE_MODEL_TYPE == \"xlm-roberta-base\" else \"haryos_model_large\"\n",
    "print(f\"Using models from: {MODELS_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Weights & Select Models\n",
    "\n",
    "We use the `WeightCalculator` to automatically find the most similar languages and calculate their mixing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Merge Configuration\n",
    "config = MergeConfig(\n",
    "    mode=MERGE_MODE,\n",
    "    target_lang=TARGET_LANG,\n",
    "    base_model=BASE_MODEL_TYPE,\n",
    "    num_languages=NUM_LANGUAGES,\n",
    "    similarity_type=SIMILARITY_TYPE,\n",
    "    include_target=INCLUDE_TARGET,\n",
    "    base_model_dir=os.path.join(project_root, MODELS_ROOT),\n",
    "    # Additional params for advanced methods\n",
    "    similarity_source=\"dense\", # Use dense for on-the-fly calculation if needed\n",
    "    top_k=20,\n",
    "    sinkhorn_iters=20\n",
    ")\n",
    "\n",
    "# Initialize Weight Calculator\n",
    "calculator = WeightCalculatorFactory.create_calculator(MERGE_MODE)\n",
    "\n",
    "# Calculate Weights\n",
    "try:\n",
    "    models_and_weights, base_model_info = calculator.calculate_weights(config)\n",
    "    print(\"\\nSelected Models and Weights:\")\n",
    "    print(f\"Base Model: {base_model_info.model_name} (Weight: {base_model_info.weight:.4f})\")\n",
    "    for path, info in models_and_weights.items():\n",
    "        print(f\" - {info.locale}: {info.weight:.4f} ({path})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating weights: {e}\")\n",
    "    # Fallback or exit logic here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform Merging\n",
    "\n",
    "Now we merge the selected models using the specified strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model Merger\n",
    "merger = ModelMerger(config)\n",
    "\n",
    "# Perform Merge\n",
    "print(f\"Merging models using {MERGE_MODE} strategy...\")\n",
    "merged_model, tokenizer = merger.merge_models(models_and_weights, base_model_info)\n",
    "print(\"Merge complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Merged Model\n",
    "\n",
    "Save the merged model to the `merged_models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Output Manager\n",
    "output_manager = OutputManager(project_root, merged_models_dir=\"merged_models\")\n",
    "\n",
    "# Save Model\n",
    "output_dir = output_manager.save_model_and_details(\n",
    "    merged_model, \n",
    "    tokenizer, \n",
    "    config, \n",
    "    models_and_weights, \n",
    "    base_model_info\n",
    ")\n",
    "\n",
    "print(f\"Merged model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Evaluate the merged model on the MASSIVE dataset for the target locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating merged model on {TARGET_LANG}...\")\n",
    "\n",
    "# Create results directory\n",
    "results_folder = create_results_folder(\n",
    "    base_model=output_dir, \n",
    "    locale=TARGET_LANG, \n",
    "    prefix=f\"{MERGE_MODE}_{NUM_LANGUAGES}lang_{SIMILARITY_TYPE}\")\n",
    "\n",
    "# Run Evaluation\n",
    "results = evaluate_specific_model(\n",
    "    model_name=output_dir, \n",
    "    locale=TARGET_LANG, \n",
    "    eval_folder=results_folder\n",
    ")\n",
    "\n",
    "if results:\n",
    "    save_evaluation_results(results, results_folder)\n",
    "    print(f\"\\nAccuracy: {results['performance']['accuracy']:.4f}\")\n",
    "    print(f\"Results saved to: {results_folder}\")\n",
    "else:\n",
    "    print(\"Evaluation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup (Optional)\n",
    "\n",
    "Remove the merged model to save space if you only care about the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(output_dir)\n",
    "# print(f\"Cleaned up {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manual Verification (No Library Imports)\n",
    "\n",
    "The following cells perform the exact same steps (Calculate Weights -> Merge -> Evaluate) but implement the logic manually within the notebook cells.\n",
    "\n",
    "This ensures that the library functions are working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Helper functions for manual weight calculation\n",
    "def manual_sinkhorn_normalize(matrix, iterations=20, epsilon=1e-9):\n",
    "    result = matrix.copy()\n",
    "    for _ in range(iterations):\n",
    "        # Normalize rows\n",
    "        result /= (result.sum(axis=1, keepdims=True) + epsilon)\n",
    "        # Normalize cols\n",
    "        result /= (result.sum(axis=0, keepdims=True) + epsilon)\n",
    "    return result\n",
    "\n",
    "def manual_filter_top_k(matrix, k):\n",
    "    sparse_matrix = matrix.copy()\n",
    "    # Zero out diagonal (self-similarity) based on library implementation\n",
    "    np.fill_diagonal(sparse_matrix, 0)\n",
    "    \n",
    "    num_rows = sparse_matrix.shape[0]\n",
    "    for i in range(num_rows):\n",
    "        row = sparse_matrix[i, :]\n",
    "        if len(row) > k:\n",
    "            # Find k-th largest value\n",
    "            kth_largest = np.partition(row, -k)[-k]\n",
    "            # Zero out anything smaller than it\n",
    "            row[row < kth_largest] = 0\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Manual Weight Calculation\n",
    "print(\"--- Manual Weight Calculation ---\")\n",
    "\n",
    "# Load Matrix\n",
    "sim_matrix_path = \"language_similarity_matrix_unified.csv\"\n",
    "df = pd.read_csv(sim_matrix_path, index_col=0)\n",
    "\n",
    "# Deduplicate\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "\n",
    "# Select Target\n",
    "manual_target_locale = TARGET_LANG # Use same var from above\n",
    "if manual_target_locale not in df.index:\n",
    "    raise ValueError(f\"{manual_target_locale} not found in matrix\")\n",
    "\n",
    "# Process Matrix\n",
    "matrix_val = df.values\n",
    "sparse_matrix = manual_filter_top_k(matrix_val, k=20)\n",
    "normalized_matrix = manual_sinkhorn_normalize(sparse_matrix, iterations=20)\n",
    "\n",
    "# Reconstruct DataFrame\n",
    "processed_df = pd.DataFrame(normalized_matrix, index=df.index, columns=df.columns)\n",
    "target_weights = processed_df.loc[manual_target_locale]\n",
    "\n",
    "# Get Top-K Weights (descending)\n",
    "top_n_weights = target_weights[target_weights > 0].sort_values(ascending=False)\n",
    "\n",
    "# Filter for num_languages (excluding target itself if ET mode)\n",
    "if not INCLUDE_TARGET:\n",
    "    top_n_weights = top_n_weights[top_n_weights.index != manual_target_locale]\n",
    "\n",
    "top_n_weights = top_n_weights[:NUM_LANGUAGES]\n",
    "\n",
    "# Normalize to sum to 1.0 for linear merge\n",
    "total_weight = top_n_weights.sum()\n",
    "final_weights = top_n_weights / total_weight\n",
    "\n",
    "print(f\"Manual Weights for {manual_target_locale}:\")\n",
    "print(final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Manual Merging\n",
    "print(\"\\n--- Manual Merging ---\")\n",
    "\n",
    "base_model_name = BASE_MODEL_TYPE\n",
    "models_root_dir = MODELS_ROOT\n",
    "\n",
    "# Identify the first model to use as the architecture base\n",
    "first_locale = final_weights.index[0]\n",
    "first_model_path = os.path.join(project_root, models_root_dir, f\"{base_model_name}_massive_k_{first_locale}\")\n",
    "\n",
    "print(f\"Initializing base architecture from: {first_model_path}\")\n",
    "config = AutoConfig.from_pretrained(first_model_path)\n",
    "manual_merged_model = AutoModelForSequenceClassification.from_config(config)\n",
    "merged_state_dict = manual_merged_model.state_dict()\n",
    "\n",
    "# Reset to zero\n",
    "for key in merged_state_dict:\n",
    "    if merged_state_dict[key].dtype.is_floating_point:\n",
    "        merged_state_dict[key].zero_()\n",
    "\n",
    "# Weighted Sum\n",
    "for locale, weight in final_weights.items():\n",
    "    model_path = os.path.join(project_root, models_root_dir, f\"{base_model_name}_massive_k_{locale}\")\n",
    "    print(f\"Loading {model_path} (w={weight:.4f})\")\n",
    "    \n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    except OSError:\n",
    "        print(f\"Warning: Could not load {model_path}, skipping...\")\n",
    "        continue\n",
    "        \n",
    "    sd = model.state_dict()\n",
    "    for key in merged_state_dict:\n",
    "        if key in sd and sd[key].dtype.is_floating_point:\n",
    "            merged_state_dict[key] += sd[key] * weight\n",
    "\n",
    "# Load merged weights back\n",
    "manual_merged_model.load_state_dict(merged_state_dict)\n",
    "print(\"Manual Merge Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Manual Evaluation\n",
    "print(f\"\\n--- Manual Evaluation on {manual_target_locale} ---\")\n",
    "\n",
    "dataset = load_dataset(\"AmazonScience/massive\", manual_target_locale, split=\"test\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(first_model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "manual_merged_model.to(device)\n",
    "manual_merged_model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for example in tqdm(dataset, desc=\"Evaluating\"):\n",
    "    inputs = tokenizer(example['utt'], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = manual_merged_model(**inputs)\n",
    "        pred_idx = outputs.logits.argmax(dim=-1).item()\n",
    "        \n",
    "    # MASSIVE uses integer intent labels\n",
    "    if pred_idx == example['intent']:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\nManual Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7af15f",
   "metadata": {},
   "source": [
    "# 8. Angle Between Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2cc82e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_locale = ['mn-MN', 'sq-AL', 'vi-VN', 'id-ID', 'th-TH', 'ka-GE', 'pt-PT', 'de-DE', 'my-MM', 'af-ZA', 'hu-HU', 'hi-IN', 'kn-IN', 'en-US', 'pl-PL', 'it-IT', 'ro-RO', 'ru-RU', 'is-IS', 'ur-PK', 'zh-TW', 'ta-IN', 'sl-SL', 'da-DK', 'fr-FR', 'es-ES', 'am-ET', 'ar-SA', 'fi-FI', 'jv-ID', 'hy-AM', 'ml-IN', 'ko-KR', 'nl-NL', 'tr-TR', 'lv-LV', 'tl-PH', 'el-GR', 'az-AZ', 'fa-IR', 'km-KH', 'sw-KE', 'te-IN', 'ms-MY', 'ja-JP', 'nb-NO', 'cy-GB', 'ca-ES', 'bn-BD']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "haryos_model = glob(\"haryos_model/*\")\n",
    "haryos_model_large = glob(\"haryos_model_large/*\")\n",
    "list_locale = [path.split(\"_\")[-1] for path in haryos_model]\n",
    "print(f\"{list_locale = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ec9f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(90.)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# arccos(0) in degrees\n",
    "torch.rad2deg(torch.acos(torch.tensor(0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e0a41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.), tensor(1.0000))\n"
     ]
    }
   ],
   "source": [
    "# Shape of weight = (64, 64)\n",
    "\n",
    "dim = 64\n",
    "example_tensor_1 = torch.randn((dim, dim))\n",
    "example_tensor_2 = torch.randn((dim, dim))\n",
    "\n",
    "def angle(weight_1: torch.Tensor, weight_2: torch.Tensor) -> torch.Tensor:\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(\n",
    "        weight_1.view(-1),\n",
    "        weight_2.view(-1),\n",
    "        dim=0\n",
    "    )\n",
    "    angle_rad = torch.acos(torch.clamp(cos_sim, -1.0, 1.0))\n",
    "    degree = torch.rad2deg(angle_rad)\n",
    "    return degree, cos_sim\n",
    "\n",
    "print(angle(example_tensor_1, example_tensor_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574335a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=60, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "finetuned_model_1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    haryos_model[0]\n",
    ")\n",
    "finetuned_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01affab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLMRobertaModel\n",
    "\n",
    "base_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "base_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
